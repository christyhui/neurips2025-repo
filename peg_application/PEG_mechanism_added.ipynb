{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b7c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import google.protobuf\n",
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94b8379",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "\n",
    "import ast\n",
    "from itertools import combinations\n",
    "from numpy.linalg import det\n",
    "\n",
    "pd.set_option('display.max_rows', None)  \n",
    "pd.set_option('display.max_columns', None) \n",
    "pd.set_option('display.width', None)  \n",
    "pd.set_option('display.max_colwidth', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ec8b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df_oqwen = pd.read_csv('../Data/arc_policy_df_oqwen_7B.csv')\n",
    "temp_df_llama2 = pd.read_csv('../Data/arc_policy_df_deepseek_llm_7b.csv')\n",
    "temp_df_qwen = pd.read_csv('../Data/arc_policy_df_deepseek_qwen_7B.csv')\n",
    "temp_df_gemma_7B = pd.read_csv('../Data/arc_policy_df_gemma-7b-it.csv') \n",
    "temp_df_mistral_7B = pd.read_csv('../Data/arc_policy_df_Mistral-7B-Instruct.csv') \n",
    "temp_df_ai_Yi_9B = pd.read_csv('../Data/arc_policy_df_ai_Yi_9B.csv')\n",
    "temp_df_openchat_7B= pd.read_csv(\"../Data/arc_policy_df_openchat_7B.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb8f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_df_qwen[\"answer_letter\"] = temp_df_qwen[\"answerKey\"]\n",
    "temp_df_oqwen[\"answer_letter\"] = temp_df_oqwen[\"answerKey\"]\n",
    "temp_df_llama2[\"answer_letter\"] = temp_df_llama2[\"answerKey\"]\n",
    "temp_df_gemma_7B[\"answer_letter\"] = temp_df_gemma_7B[\"answerKey\"]\n",
    "temp_df_mistral_7B[\"answer_letter\"] = temp_df_mistral_7B[\"answerKey\"]\n",
    "temp_df_ai_Yi_9B[\"answer_letter\"] = temp_df_ai_Yi_9B[\"answerKey\"]\n",
    "temp_df_openchat_7B[\"answer_letter\"] = temp_df_openchat_7B[\"answerKey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8ebd368",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer_alignment(df_qwen, df_oqwen, df_llama2, answer_col='disc_init_answer', label_col='answer_letter'):\n",
    "    \"\"\"\n",
    "    Compare predicted answers and ground truth across three DataFrames.\n",
    "    \n",
    "    Args:\n",
    "        df_qwen (DataFrame): First model's dataframe (e.g., Qwen).\n",
    "        df_oqwen (DataFrame): Second model's dataframe (e.g., Oqwen).\n",
    "        df_llama2 (DataFrame): Third model's dataframe (e.g., Llama2).\n",
    "        answer_col (str): Name of the model prediction column (default 'disc_init_answer').\n",
    "        label_col (str): Name of the ground truth column (default 'answer_letter').\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: Table summarizing counts and accuracies.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_n = df_oqwen.shape[0]\n",
    "    \n",
    "    qwen_corr = (df_qwen[answer_col] == df_qwen[label_col]).sum()\n",
    "    oqwen_corr = (df_oqwen[answer_col] == df_oqwen[label_col]).sum()\n",
    "    llama2_corr = (df_llama2[answer_col] == df_llama2[label_col]).sum()\n",
    "\n",
    "    qwen_llama2_oqwen = ((df_oqwen[answer_col] == df_llama2[answer_col]) & (df_llama2[answer_col] == df_llama2[label_col]) & (df_qwen[answer_col] == df_oqwen[answer_col])).sum()\n",
    "    \n",
    "    results_d = {\n",
    "        \"Metric\": [\n",
    "            \"Total Question #\",\n",
    "            \"oqwen correct answers\",\n",
    "            \"qwen correct answers\",\n",
    "            \"llama2 correct answers\",\n",
    "            \"qwen llama2 oqwen\", \n",
    "        ],\n",
    "        \"Count\": [\n",
    "            total_n,\n",
    "            oqwen_corr,\n",
    "            qwen_corr,\n",
    "            llama2_corr,\n",
    "            qwen_llama2_oqwen,\n",
    "        ]  \n",
    "    }\n",
    "    \n",
    "    results_df = pd.DataFrame(results_d)\n",
    "    results_df[\"Accuracy\"] = (results_df[\"Count\"] / total_n).apply(lambda x: f\"{x:.0%}\")\n",
    "    \n",
    "    return results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97411313",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "COMMON_COLUMNS = [\"disc_init_policy\", \"gen_init_policy\", \"answer_letter\", \"disc_answer\", \"gen_answer\"]\n",
    "RENAME_MAPPING = {\"disc_answer\": \"ED_consensus\", \"gen_answer\": \"EG_consensus\"}\n",
    "\n",
    "def process_dataframe(temp_df):\n",
    "    \"\"\"Process a dataframe by selecting columns and renaming them.\"\"\"\n",
    "    df = temp_df[COMMON_COLUMNS].copy()\n",
    "    df.rename(columns=RENAME_MAPPING, inplace=True)\n",
    "    return df\n",
    "\n",
    "# ARC Challenge datasets\n",
    "df_oqwen = process_dataframe(temp_df_oqwen)\n",
    "df_deepseekqwen = process_dataframe(temp_df_qwen)\n",
    "df_deepseekllama = process_dataframe(temp_df_llama2)\n",
    "df_gemma_7B = process_dataframe(temp_df_gemma_7B)\n",
    "df_llama3_8B = process_dataframe(temp_df_llama3_8b)\n",
    "df_mistral_7B = process_dataframe(temp_df_mistral_7B)\n",
    "df_ai_Yi_9B = process_dataframe(temp_df_ai_Yi_9B)\n",
    "df_openchat_7B = process_dataframe(temp_df_openchat_7B)\n",
    "df_zephyer = process_dataframe(temp_df_zephyer)\n",
    "df_phi2 = process_dataframe(temp_df_phi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7ceb640",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def parse_np_float_string(s):\n",
    "    if isinstance(s, dict):\n",
    "        return s\n",
    "    if not isinstance(s, str):\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        # 将 np.float32(0.12345) 替换为 0.12345\n",
    "        s_clean = re.sub(r'np\\.float32\\((.*?)\\)', r'\\1', s)\n",
    "        return ast.literal_eval(s_clean)\n",
    "    except Exception as e:\n",
    "        print(\"Still failed to parse:\", s)\n",
    "        return {}\n",
    "\n",
    "\n",
    "for df in [\n",
    "    df_oqwen, df_deepseekqwen, df_deepseekllama,\n",
    "    df_gemma_7B , df_llama3_8B, df_mistral_7B,\n",
    "    df_ai_Yi_9B, df_openchat_7B, df_zephyer, df_phi2\n",
    "]:\n",
    "    df[\"disc_init_policy\"] = df[\"disc_init_policy\"].apply(parse_np_float_string)\n",
    "    df[\"gen_init_policy\"] = df[\"gen_init_policy\"].apply(parse_np_float_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47515c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [\n",
    "    df_oqwen, df_deepseekqwen, df_deepseekllama,\n",
    "    df_gemma_7B , df_llama3_8B, df_mistral_7B,\n",
    "    df_ai_Yi_9B, df_openchat_7B, df_zephyer, df_phi2\n",
    "]:\n",
    "    for col in [\"disc_init_policy\", \"gen_init_policy\"]:\n",
    "        if df[col].dtype == object and isinstance(df[col].iloc[0], str):\n",
    "            df[col] = df[col].apply(ast.literal_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc917f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalized the incorrect and correct policy in determinator\n",
    "def apply_softmax(choice_dict):\n",
    "    # Extract correct and incorrect values for all choices\n",
    "    \n",
    "    correct_values = np.array([v['correct'] for v in choice_dict.values()])\n",
    "    incorrect_values = np.array([v['incorrect'] for v in choice_dict.values()])\n",
    "    \n",
    "    # Define softmax function\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "    \n",
    "    softmax_correct = softmax(correct_values)\n",
    "    softmax_incorrect = softmax(incorrect_values)\n",
    "    \n",
    "    result = {}\n",
    "    for i, choice in enumerate(choice_dict.keys()):\n",
    "        result[choice] = {\n",
    "            'correct': softmax_correct[i],\n",
    "            'incorrect': softmax_incorrect[i]\n",
    "        }\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "for df in [\n",
    "    df_oqwen, df_deepseekqwen, df_deepseekllama,\n",
    "    df_gemma_7B , df_llama3_8B, df_mistral_7B,\n",
    "    df_ai_Yi_9B, df_openchat_7B, df_zephyer, df_phi2\n",
    "]:\n",
    "    df[\"disc_init_policy_refine\"] = df[\"disc_init_policy\"].apply(apply_softmax)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41b3904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_probable_letter(answer_dict):\n",
    "    \n",
    "    if answer_dict is None:\n",
    "        return None \n",
    "    \n",
    "    letter, _ = max(answer_dict.items(), key=lambda item: item[1]['correct'])\n",
    "    return letter\n",
    "\n",
    "for df in [\n",
    "    df_oqwen, df_deepseekqwen, df_deepseekllama,\n",
    "    df_gemma_7B , df_llama3_8B, df_mistral_7B,\n",
    "    df_ai_Yi_9B, df_openchat_7B, df_zephyer, df_phi2\n",
    "]:\n",
    "    df[\"disc_init_answer\"] = df[\"disc_init_policy\"].apply(get_most_probable_letter)\n",
    "    df[\"disc_init_refine_answer\"] = df[\"disc_init_policy_refine\"].apply(get_most_probable_letter)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f057109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df in [\n",
    "#     df_oqwen, df_deepseekqwen, df_deepseekllama,\n",
    "#     df_gemma_7B , df_llama3_8B, df_mistral_7B\n",
    "# ]:\n",
    "#     display(df.head(8))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e3321ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reweight_joint(row):\n",
    "    plm_y_given_xl = row['gen_init_policy'] \n",
    "    plm_l_given_xy = row['disc_init_policy']  \n",
    "\n",
    "    new_joint = {}\n",
    "    for y, label_probs in plm_y_given_xl.items():\n",
    "        new_joint[y] = {}\n",
    "        for label, p1 in label_probs.items():\n",
    "            p2 = plm_l_given_xy.get(label, {}).get(y, 0.0)\n",
    "            new_joint[y][label] = p1 * p2\n",
    "    return new_joint\n",
    "\n",
    "\n",
    "def get_max_correct_label(mi_dict):\n",
    "    return max(mi_dict['correct'], key=mi_dict['correct'].get)\n",
    "\n",
    "\n",
    "for df in [\n",
    "    df_oqwen, df_deepseekqwen, df_deepseekllama,\n",
    "    df_gemma_7B , df_llama3_8B, df_mistral_7B, df_zephyer, df_phi2\n",
    "]:\n",
    "    df[\"MI\"] = df.apply(reweight_joint, axis=1)\n",
    "    df[\"MI_answer\"] = df[\"MI\"].apply(get_max_correct_label)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a25a47ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_majority_vote_fixed(df_qwen, df_oqwen, df_llama2, answer_col='disc_init_refine_answer', label_col='answer_letter'):\n",
    "    \"\"\"\n",
    "    Compare three models' predictions with ground truth, and compute majority vote accuracy.\n",
    "    This function recalculates model correctness based on combined_df only.\n",
    "    \n",
    "    Args:\n",
    "        df_qwen (DataFrame): First model's dataframe (e.g., Qwen).\n",
    "        df_oqwen (DataFrame): Second model's dataframe (e.g., OQwen).\n",
    "        df_llama2 (DataFrame): Third model's dataframe (e.g., Llama2).\n",
    "        answer_col (str): Name of the model prediction column (default 'disc_init_refine_answer').\n",
    "        label_col (str): Name of the ground truth column (default 'answer_letter').\n",
    "        \n",
    "    Returns:\n",
    "        combined_df (DataFrame): DataFrame containing all predictions and majority votes.\n",
    "        results_df (DataFrame): Summary table of counts and accuracies.\n",
    "    \"\"\"\n",
    "\n",
    "    # Combine predictions\n",
    "    combined_df = pd.DataFrame({\n",
    "        \"question_id\": df_oqwen.index,\n",
    "        \"correct_answer\": df_oqwen[label_col],   # Use OQwen's label (or consistent label)\n",
    "        \"qwen_pred\": df_qwen[answer_col],\n",
    "        \"oqwen_pred\": df_oqwen[answer_col],\n",
    "        \"llama2_pred\": df_llama2[answer_col]\n",
    "    })\n",
    "    \n",
    "    # Majority vote\n",
    "    def get_majority_vote(row):\n",
    "        votes = [row[\"qwen_pred\"], row[\"oqwen_pred\"], row[\"llama2_pred\"]]\n",
    "        vote_counts = Counter(votes)\n",
    "        majority_answers = [ans for ans, count in vote_counts.items() if count >= 2]\n",
    "        if not majority_answers:\n",
    "            return None\n",
    "        return majority_answers[0]\n",
    "    \n",
    "    combined_df[\"majority_vote\"] = combined_df.apply(get_majority_vote, axis=1)\n",
    "    \n",
    "    # Check correctness\n",
    "    combined_df[\"qwen_correct\"] = combined_df[\"qwen_pred\"] == combined_df[\"correct_answer\"]\n",
    "    combined_df[\"oqwen_correct\"] = combined_df[\"oqwen_pred\"] == combined_df[\"correct_answer\"]\n",
    "    combined_df[\"llama2_correct\"] = combined_df[\"llama2_pred\"] == combined_df[\"correct_answer\"]\n",
    "    combined_df[\"is_majority_correct\"] = combined_df[\"majority_vote\"] == combined_df[\"correct_answer\"]\n",
    "\n",
    "    # Summarize\n",
    "    total_questions = len(combined_df)\n",
    "    qwen_corr = combined_df[\"qwen_correct\"].sum()\n",
    "    oqwen_corr = combined_df[\"oqwen_correct\"].sum()\n",
    "    llama2_corr = combined_df[\"llama2_correct\"].sum()\n",
    "    majority_correct = combined_df[\"is_majority_correct\"].sum()\n",
    "\n",
    "    results_d = {\n",
    "        \"Metric\": [\n",
    "            \"Total Questions\",\n",
    "            \"Qwen Accuracy\",\n",
    "            \"OQwen Accuracy\",\n",
    "            \"Llama2 Accuracy\",\n",
    "            \"Majority Vote (≥2 models) Accuracy\",\n",
    "        ],\n",
    "        \"Count\": [\n",
    "            total_questions,\n",
    "            qwen_corr,\n",
    "            oqwen_corr,\n",
    "            llama2_corr,\n",
    "            majority_correct,\n",
    "        ],\n",
    "        \"Accuracy\": [\n",
    "            \"100%\",\n",
    "            f\"{(qwen_corr / total_questions) * 100:.2f}%\",\n",
    "            f\"{(oqwen_corr / total_questions) * 100:.2f}%\",\n",
    "            f\"{(llama2_corr / total_questions) * 100:.2f}%\",\n",
    "            f\"{(majority_correct / total_questions) * 100:.2f}%\",\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    results_df = pd.DataFrame(results_d)\n",
    "    \n",
    "    return combined_df, results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83dd3ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_majority_vote(\n",
    "    model_dfs: dict[str, pd.DataFrame],\n",
    "    answer_col: str = 'disc_init_refine_answer',\n",
    "    label_col: str = 'answer_letter'\n",
    "):\n",
    "    \"\"\"\n",
    "    Compare multiple models' predictions, compute majority vote accuracy, and summarize results.\n",
    "\n",
    "    Args:\n",
    "        model_dfs (dict): A dictionary where keys are model names (str) and\n",
    "                          values are their corresponding DataFrames (pd.DataFrame).\n",
    "        answer_col (str): The name of the column containing model predictions.\n",
    "        label_col (str): The name of the column containing the ground truth answer.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - combined_df (pd.DataFrame): DataFrame with all predictions and votes.\n",
    "            - results_df (pd.DataFrame): A summary table of accuracies.\n",
    "    \"\"\"\n",
    "    model_names = list(model_dfs.keys())\n",
    "    if not model_names:\n",
    "        raise ValueError(\"The model_dfs dictionary cannot be empty.\")\n",
    "\n",
    "    num_models = len(model_names)\n",
    "    \n",
    "    # --- 1. Combine Predictions ---\n",
    "    # Use the first model's DataFrame to set up the base\n",
    "    base_df_name = model_names[0]\n",
    "    combined_df = pd.DataFrame({\n",
    "        \"question_id\": model_dfs[base_df_name].index,\n",
    "        \"correct_answer\": model_dfs[base_df_name][label_col]\n",
    "    })\n",
    "\n",
    "    # Add each model's prediction as a new column\n",
    "    pred_cols = []\n",
    "    for name, df in model_dfs.items():\n",
    "        pred_col_name = f\"{name}_pred\"\n",
    "        combined_df[pred_col_name] = df[answer_col]\n",
    "        pred_cols.append(pred_col_name)\n",
    "\n",
    "    # --- 2. Calculate Majority Vote ---\n",
    "    def get_majority_vote(row):\n",
    "        # A true majority means a count greater than half the number of models\n",
    "        majority_threshold = math.floor(num_models / 2) + 1\n",
    "        \n",
    "        votes = [row[col] for col in pred_cols if pd.notna(row[col])]\n",
    "        if not votes:\n",
    "            return None\n",
    "        \n",
    "        vote_counts = Counter(votes)\n",
    "        most_common_ans, highest_count = vote_counts.most_common(1)[0]\n",
    "        \n",
    "        # Return the answer only if it meets the majority threshold\n",
    "        return most_common_ans if highest_count >= majority_threshold else None\n",
    "\n",
    "    combined_df[\"majority_vote\"] = combined_df.apply(get_majority_vote, axis=1)\n",
    "\n",
    "    # --- 3. Check Correctness ---\n",
    "    for name in model_names:\n",
    "        combined_df[f\"{name}_correct\"] = combined_df[f\"{name}_pred\"] == combined_df[\"correct_answer\"]\n",
    "    \n",
    "    combined_df[\"is_majority_correct\"] = combined_df[\"majority_vote\"] == combined_df[\"correct_answer\"]\n",
    "\n",
    "    # --- 4. Summarize Results ---\n",
    "    total_questions = len(combined_df)\n",
    "    \n",
    "    metrics = [\"Total Questions\"]\n",
    "    counts = [total_questions]\n",
    "    accuracies = [f\"100%\"]\n",
    "\n",
    "    # Add results for each individual model\n",
    "    for name in model_names:\n",
    "        correct_count = combined_df[f\"{name}_correct\"].sum()\n",
    "        metrics.append(f\"{name} Accuracy\")\n",
    "        counts.append(correct_count)\n",
    "        accuracies.append(f\"{(correct_count / total_questions) * 100:.2f}%\")\n",
    "        \n",
    "    # Add final majority vote result\n",
    "    majority_correct = combined_df[\"is_majority_correct\"].sum()\n",
    "    majority_threshold_display = math.floor(num_models / 2)\n",
    "    metrics.append(f\"Majority Vote (>{majority_threshold_display} models) Accuracy\")\n",
    "    counts.append(majority_correct)\n",
    "    accuracies.append(f\"{(majority_correct / total_questions) * 100:.2f}%\")\n",
    "\n",
    "    results_df = pd.DataFrame({\"Metric\": metrics, \"Count\": counts, \"Accuracy\": accuracies})\n",
    "    \n",
    "    return combined_df, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70c92c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "class Discriminator:\n",
    "    def __init__(self, init_policy):\n",
    "        \"\"\"Initialize with given probability dictionary.\"\"\"\n",
    "        self.policy = init_policy \n",
    "\n",
    "    def respond(self, choice, max_choice):\n",
    "        \"\"\"Sample a response ('correct' or 'incorrect') for a given answer choice.\"\"\"\n",
    "        prob_correct = self.policy[choice]['correct']\n",
    "        res = \"correct\" if choice == max_choice else \"incorrect\"    \n",
    "#         if choice == max_choice:\n",
    "#             res = \"correct\"\n",
    "#         else:\n",
    "#             res = np.random.choice([\"correct\", \"incorrect\"], p=[prob_correct, 1- prob_correct])\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def log_gradient(self, choice, response):\n",
    "\n",
    "        if response == \"correct\":\n",
    "            grad_correct = 1 / self.policy[choice]['correct']\n",
    "            grad_incorrect = -1 / self.policy[choice]['correct']\n",
    "        else:\n",
    "            grad_correct = -1 / self.policy[choice]['incorrect']\n",
    "            grad_incorrect = 1 / self.policy[choice]['incorrect']\n",
    "\n",
    "        return {'correct': grad_correct, 'incorrect': grad_incorrect}\n",
    "\n",
    "    def get_max_correct_choice(self):\n",
    "        \"\"\"Return the choice with highest 'correct' probability\"\"\"\n",
    "        return max(self.policy.items(), key=lambda x: x[1]['correct'])[0]\n",
    "    \n",
    "    def update_policy(self, choice, response, reward, learning_rate=0.1):\n",
    "        grads = self.log_gradient(choice, response)\n",
    "        prob_correct = self.policy[choice]['correct']\n",
    "        prob_incorrect = self.policy[choice]['incorrect']\n",
    "        \n",
    "        log_prob_correct = np.log(prob_correct) + learning_rate * reward * grads['correct']\n",
    "        log_prob_incorrect = np.log(prob_incorrect) + learning_rate * reward * grads['incorrect']\n",
    "\n",
    "        max_log = max(log_prob_correct, log_prob_incorrect)\n",
    "        log_prob_correct -= max_log\n",
    "        log_prob_incorrect -= max_log\n",
    "\n",
    "        exp_correct = np.exp(log_prob_correct)\n",
    "        exp_incorrect = np.exp(log_prob_incorrect)\n",
    "        total = exp_correct + exp_incorrect\n",
    "        self.policy[choice]['correct'] = exp_correct / total\n",
    "        self.policy[choice]['incorrect'] = exp_incorrect / total\n",
    "        \n",
    "        self.policy[choice]['correct'] = np.clip(self.policy[choice]['correct'], 1e-6, 1 - 1e-6)\n",
    "        self.policy[choice]['incorrect'] = 1 - self.policy[choice]['correct']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "label_to_index = {'correct': 0, 'incorrect': 1}\n",
    "\n",
    "def normalize_rewards(dmi_dict):\n",
    "    values = np.array(list(dmi_dict.values()))\n",
    "    mean = np.mean(values)\n",
    "    std = np.std(values) + 1e-8\n",
    "    return {k: (v - mean) / std for k, v in dmi_dict.items()}\n",
    "\n",
    "\n",
    "        \n",
    "def generate_matrix(part_data, i, j):\n",
    "    matrix = np.zeros((2, 2), dtype=int)\n",
    "    for row in part_data:\n",
    "        row_i = label_to_index[row[i]]\n",
    "        row_j = label_to_index[row[j]]\n",
    "        matrix[row_i][row_j] += 1\n",
    "    return matrix\n",
    "\n",
    "\n",
    "def calculate_dmi_score(part1_matrix, part2_matrix):\n",
    "    det_part1 = np.linalg.det(part1_matrix)\n",
    "    det_part2 = np.linalg.det(part2_matrix)\n",
    "    dmi_score = det_part1 * det_part2\n",
    "    return dmi_score\n",
    "\n",
    "def compute_dmi_payments(responses):\n",
    "    \"\"\"Compute DMI payments using determinants of agreement matrices.\"\"\"\n",
    "    n = len(responses)\n",
    "    split1 = responses[ : (n + 1)//2]  \n",
    "    split2 = responses[(n + 1)//2 : ]  \n",
    "    splits = {'Part1': split1, 'Part2': split2}\n",
    "    \n",
    "    n_d = len(responses[0])\n",
    "    d_dmi_dict = {i : 0 for i in range(n_d)}\n",
    "    for i, j in combinations(range(n_d), 2):\n",
    "        part1_matrix = generate_matrix(splits[\"Part1\"], i, j)\n",
    "        part2_matrix = generate_matrix(splits[\"Part2\"], i, j)\n",
    "        dmi_score = calculate_dmi_score(part1_matrix, part2_matrix)\n",
    "        d_dmi_dict[i] += dmi_score\n",
    "        d_dmi_dict[j] += dmi_score\n",
    "        \n",
    "    return d_dmi_dict\n",
    "\n",
    "def batch_update_discriminators(\n",
    "    discriminator_dfs: dict, # Pass a dictionary of DataFrames\n",
    "    choice_df,\n",
    "    T_steps,\n",
    "    batch_size=8,\n",
    "    learning_rate=0.1\n",
    "):\n",
    "    \"\"\"\n",
    "    Batch update discriminator policies for a variable number of discriminators.\n",
    "    \n",
    "    Args:\n",
    "        discriminator_dfs (dict): A dictionary where keys are model names (str) and\n",
    "                                  values are their corresponding DataFrames.\n",
    "        choice_df (DataFrame): DataFrame used for selecting initial choices.\n",
    "        batch_size (int): Number of rows to update in each batch.\n",
    "        T_steps (int): Number of response-update iterations per batch.\n",
    "        learning_rate (float): Learning rate for policy update.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A dictionary containing the updated DataFrames, with the same keys.\n",
    "    \"\"\"\n",
    "    if batch_size < 4:\n",
    "        print(\"Warning: batch size should be >= 4 for DMI calculation.\")\n",
    "    \n",
    "    # Extract model names and a list of DataFrames for indexed processing\n",
    "    model_names = list(discriminator_dfs.keys())\n",
    "    df_list = [df.copy() for df in discriminator_dfs.values()] # Work on copies to avoid modifying original input dict\n",
    "\n",
    "\n",
    "    if not df_list:\n",
    "        return {} # Return empty dict if input is empty\n",
    "\n",
    "    num_discriminators = len(df_list)\n",
    "    n_rows = df_list[0].shape[0]\n",
    "\n",
    "    # Initialize the column for updated policies in each DataFrame\n",
    "    for df in df_list:\n",
    "        df[\"updated_disc_policy\"] = None\n",
    "\n",
    "    for i in range(0, n_rows, batch_size):\n",
    "        current_batch_size = min(batch_size, n_rows - i)\n",
    "        if current_batch_size < 4: continue\n",
    "\n",
    "        # Initialize discriminators from the list of DataFrames\n",
    "        discriminators = [\n",
    "            [Discriminator(copy.deepcopy(df.loc[i + j, 'disc_init_policy_refine'])) for j in range(current_batch_size)]\n",
    "            for df in df_list\n",
    "        ]\n",
    "\n",
    "        # Initialize choices for the batch\n",
    "        choices = [\n",
    "            max(choice_df.loc[i + j, \"gen_init_policy\"][\"correct\"],\n",
    "                key=choice_df.loc[i + j, \"gen_init_policy\"][\"correct\"].get)\n",
    "            for j in range(batch_size)\n",
    "        ]\n",
    "        \n",
    "        for _ in range(T_steps):\n",
    "            batch_responses = []\n",
    "            for j in range(current_batch_size):\n",
    "                task_responses = []\n",
    "                for d in range(num_discriminators):\n",
    "                    max_choice = discriminators[d][j].get_max_correct_choice()\n",
    "                    response = discriminators[d][j].respond(choices[j], max_choice)\n",
    "                    task_responses.append(response)\n",
    "                batch_responses.append(task_responses)\n",
    "\n",
    "            dmi_scores = compute_dmi_payments(batch_responses)\n",
    "            dmi_scores = normalize_rewards(dmi_scores)\n",
    "\n",
    "            for d in range(num_discriminators):\n",
    "                for j in range(current_batch_size):\n",
    "                    choice = choices[j]\n",
    "                    response = batch_responses[j][d]\n",
    "                    reward = dmi_scores.get(d, 0) # Use .get for safety\n",
    "                    discriminators[d][j].update_policy(choice, response, reward, learning_rate)\n",
    "\n",
    "        # Save the final updated policies back to the DataFrames in the list\n",
    "        for d in range(num_discriminators):\n",
    "            for j in range(current_batch_size):\n",
    "                df_list[d].at[i + j, \"updated_disc_policy\"] = discriminators[d][j].policy\n",
    "    \n",
    "    # Add the final answer letter based on the updated policy\n",
    "    for df in df_list:\n",
    "        df['updated_answer_letter'] = df['updated_disc_policy'].apply(get_most_probable_letter)\n",
    "\n",
    "    # Reconstruct the dictionary with the original names and updated DataFrames\n",
    "    updated_dfs_dict = {name: df for name, df in zip(model_names, df_list)}\n",
    "    \n",
    "    return updated_dfs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df9f433e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>1170</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>995</td>\n",
       "      <td>85.04%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DeepseekQwen Accuracy</td>\n",
       "      <td>1008</td>\n",
       "      <td>86.15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DeepseekLlama Accuracy</td>\n",
       "      <td>1006</td>\n",
       "      <td>85.98%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Majority Vote (&gt;1 models) Accuracy</td>\n",
       "      <td>1018</td>\n",
       "      <td>87.01%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions   1170     100%\n",
       "1                      OQwen Accuracy    995   85.04%\n",
       "2               DeepseekQwen Accuracy   1008   86.15%\n",
       "3              DeepseekLlama Accuracy   1006   85.98%\n",
       "4  Majority Vote (>1 models) Accuracy   1018   87.01%"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "temp_discriminator_dict = {\n",
    "    \"OQwen\": temp_df_oqwen,\n",
    "    \"DeepseekQwen\": temp_df_qwen,\n",
    "    \"DeepseekLlama\": temp_df_llama2,\n",
    "    # \"Gemma_7B\":  temp_df_gemma_7B,\n",
    "    # \"Llama3_8B\": temp_df_llama3_8b,\n",
    "    # \"Mistral_7B\":  temp_df_mistral_7B,\n",
    "}\n",
    "\n",
    "discriminator_dict = {\n",
    "    \"OQwen\": df_oqwen,\n",
    "    \"DeepseekQwen\": df_deepseekqwen,\n",
    "    \"DeepseekLlama\": df_deepseekllama,\n",
    "    # \"Gemma_7B\": df_gemma_7B,\n",
    "    # \"Llama3_8B\": df_llama3_8B,\n",
    "    # \"Mistral_7B\": df_mistral_7B,\n",
    "}\n",
    "\n",
    "\n",
    "discriminator_dict_update =  batch_update_discriminators(\n",
    "    discriminator_dict,\n",
    "    choice_df = df_oqwen, \n",
    "    batch_size=8,\n",
    "    T_steps=5,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "    \n",
    "combined_df_refine_update, results_df_refine_update = evaluate_majority_vote(\n",
    "    discriminator_dict_update,  \n",
    "    answer_col='updated_answer_letter', \n",
    "    label_col='answer_letter')\n",
    "\n",
    "results_df_refine_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "204aaba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration Number (T_steps)</th>\n",
       "      <th>Majority Vote Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>70.51%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>87.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>87.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>87.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>87.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>87.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>87.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>87.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>87.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>87.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>87.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>87.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>87.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>87.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>87.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>87.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>87.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>87.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>87.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>87.01%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Iteration Number (T_steps) Majority Vote Accuracy\n",
       "0                            0                 70.51%\n",
       "1                            1                 87.01%\n",
       "2                            2                 87.01%\n",
       "3                            3                 87.01%\n",
       "4                            4                 87.01%\n",
       "5                            5                 87.01%\n",
       "6                            6                 87.01%\n",
       "7                            7                 87.01%\n",
       "8                            8                 87.01%\n",
       "9                            9                 87.01%\n",
       "10                          10                 87.01%\n",
       "11                          11                 87.01%\n",
       "12                          12                 87.01%\n",
       "13                          13                 87.01%\n",
       "14                          14                 87.01%\n",
       "15                          15                 87.01%\n",
       "16                          16                 87.01%\n",
       "17                          17                 87.01%\n",
       "18                          18                 87.01%\n",
       "19                          19                 87.01%"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## ARC Challenger Iteration Test\n",
    "\n",
    "\n",
    "temp_discriminator_dict = {\n",
    "    \"OQwen\": temp_df_oqwen,\n",
    "    \"DeepseekQwen\": temp_df_qwen,\n",
    "    \"DeepseekLlama\": temp_df_llama2\n",
    "}\n",
    "\n",
    "discriminator_dict = {\n",
    "    \"OQwen\": df_oqwen,\n",
    "    \"DeepseekQwen\": df_deepseekqwen,\n",
    "    \"DeepseekLlama\": df_deepseekllama\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "majority_vote_accuracies = []\n",
    "for t_step_val in range(20):\n",
    "    discriminator_dict_update =  batch_update_discriminators(\n",
    "        discriminator_dict,\n",
    "        choice_df = df_oqwen, \n",
    "        batch_size=8,\n",
    "        T_steps=t_step_val,\n",
    "        learning_rate=0.1\n",
    "    )\n",
    "        \n",
    "    combined_df_refine_update, results_df_refine_update = evaluate_majority_vote(\n",
    "        discriminator_dict_update,  \n",
    "        answer_col='updated_answer_letter', \n",
    "        label_col='answer_letter')\n",
    "\n",
    "\n",
    "\n",
    "    last_row = results_df_refine_update.iloc[-1]\n",
    "    majority_vote_accuracy = last_row['Accuracy']\n",
    "    # print(majority_vote_accuracy)\n",
    "    majority_vote_accuracies.append(majority_vote_accuracy)\n",
    "\n",
    "iteration_numbers = list(range(0, 20))\n",
    "plot_df = pd.DataFrame({\n",
    "    'Iteration Number (T_steps)': iteration_numbers,\n",
    "    'Majority Vote Accuracy': majority_vote_accuracies\n",
    "})\n",
    "plot_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26cd8af",
   "metadata": {},
   "source": [
    "## 5 discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98705bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_discriminator_dict = {\n",
    "    \"OQwen\": temp_df_oqwen,\n",
    "    \"DeepseekQwen\": temp_df_qwen,\n",
    "    \"Llama3_8B\": temp_df_llama3_8b,\n",
    "    \"Gemma_7B\":  temp_df_gemma_7B,\n",
    "    \"Mistral_7B\":  temp_df_mistral_7B,\n",
    "}\n",
    "\n",
    "discriminator_dict = {\n",
    "    \"OQwen\": df_oqwen,\n",
    "    \"DeepseekQwen\": df_deepseekqwen,\n",
    "    \"Llama3_8B\": df_llama3_8B,\n",
    "    \"Gemma_7B\": df_gemma_7B,\n",
    "    \"Mistral_7B\": df_mistral_7B,\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1758c0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Discriminator Policy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>1170</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>970</td>\n",
       "      <td>82.91%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DeepseekQwen Accuracy</td>\n",
       "      <td>669</td>\n",
       "      <td>57.18%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama3_8B Accuracy</td>\n",
       "      <td>890</td>\n",
       "      <td>76.07%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gemma_7B Accuracy</td>\n",
       "      <td>811</td>\n",
       "      <td>69.32%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mistral_7B Accuracy</td>\n",
       "      <td>822</td>\n",
       "      <td>70.26%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Majority Vote (&gt;2 models) Accuracy</td>\n",
       "      <td>891</td>\n",
       "      <td>76.15%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions   1170     100%\n",
       "1                      OQwen Accuracy    970   82.91%\n",
       "2               DeepseekQwen Accuracy    669   57.18%\n",
       "3                  Llama3_8B Accuracy    890   76.07%\n",
       "4                   Gemma_7B Accuracy    811   69.32%\n",
       "5                 Mistral_7B Accuracy    822   70.26%\n",
       "6  Majority Vote (>2 models) Accuracy    891   76.15%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Consensus Game Policy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>1170</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>1025</td>\n",
       "      <td>87.61%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DeepseekQwen Accuracy</td>\n",
       "      <td>822</td>\n",
       "      <td>70.26%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama3_8B Accuracy</td>\n",
       "      <td>948</td>\n",
       "      <td>81.03%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gemma_7B Accuracy</td>\n",
       "      <td>796</td>\n",
       "      <td>68.03%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mistral_7B Accuracy</td>\n",
       "      <td>854</td>\n",
       "      <td>72.99%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Majority Vote (&gt;2 models) Accuracy</td>\n",
       "      <td>939</td>\n",
       "      <td>80.26%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions   1170     100%\n",
       "1                      OQwen Accuracy   1025   87.61%\n",
       "2               DeepseekQwen Accuracy    822   70.26%\n",
       "3                  Llama3_8B Accuracy    948   81.03%\n",
       "4                   Gemma_7B Accuracy    796   68.03%\n",
       "5                 Mistral_7B Accuracy    854   72.99%\n",
       "6  Majority Vote (>2 models) Accuracy    939   80.26%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Updated PEG Policy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>1170</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>1013</td>\n",
       "      <td>86.58%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DeepseekQwen Accuracy</td>\n",
       "      <td>968</td>\n",
       "      <td>82.74%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama3_8B Accuracy</td>\n",
       "      <td>1002</td>\n",
       "      <td>85.64%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gemma_7B Accuracy</td>\n",
       "      <td>976</td>\n",
       "      <td>83.42%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mistral_7B Accuracy</td>\n",
       "      <td>963</td>\n",
       "      <td>82.31%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Majority Vote (&gt;2 models) Accuracy</td>\n",
       "      <td>1011</td>\n",
       "      <td>86.41%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions   1170     100%\n",
       "1                      OQwen Accuracy   1013   86.58%\n",
       "2               DeepseekQwen Accuracy    968   82.74%\n",
       "3                  Llama3_8B Accuracy   1002   85.64%\n",
       "4                   Gemma_7B Accuracy    976   83.42%\n",
       "5                 Mistral_7B Accuracy    963   82.31%\n",
       "6  Majority Vote (>2 models) Accuracy   1011   86.41%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "combined_df_inital_results_dis, results_df_inital_dis = evaluate_majority_vote(\n",
    "    model_dfs=temp_discriminator_dict,\n",
    "    answer_col='disc_init_answer', \n",
    "    label_col='answer_letter'\n",
    ")\n",
    "print(\"\\nInitial Discriminator Policy\")\n",
    "display(results_df_inital_dis)\n",
    "\n",
    "combined_df_inital_results_ED, results_df_inital_ED  = evaluate_majority_vote(\n",
    "    model_dfs=discriminator_dict,\n",
    "     answer_col='ED_consensus', \n",
    "     label_col='answer_letter')\n",
    "\n",
    "print(\"\\nInitial Consensus Game Policy\")\n",
    "display(results_df_inital_ED)\n",
    "\n",
    "## ARC Challenger Original Qwen as problem selector\n",
    "\n",
    "discriminator_dict_update =  batch_update_discriminators(\n",
    "    discriminator_dict,\n",
    "    choice_df = df_oqwen, \n",
    "    batch_size=8,\n",
    "    T_steps=11,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "       \n",
    "combined_df_refine_update, results_df_refine_update = evaluate_majority_vote(\n",
    "    discriminator_dict_update,  \n",
    "    answer_col='updated_answer_letter', \n",
    "    label_col='answer_letter')\n",
    "\n",
    "print(\"\\n Updated PEG Policy\")\n",
    "display(results_df_refine_update)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20a04eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iteration Number (T_steps)</th>\n",
       "      <th>Majority Vote Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>75.98%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>86.92%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>86.75%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>86.58%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>86.58%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>86.58%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>86.32%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>86.32%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>86.58%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>86.07%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>86.15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>86.41%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>86.24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>86.41%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>86.24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>86.24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>86.41%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>85.98%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>86.41%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>86.15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>86.32%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>86.07%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>86.24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>86.15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>86.07%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>86.41%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>85.98%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>86.32%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>86.24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>86.24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>86.15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>86.24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>86.41%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>86.24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>86.41%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>86.15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>86.24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>86.41%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>38</td>\n",
       "      <td>86.24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>86.41%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>86.32%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>41</td>\n",
       "      <td>86.15%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>86.32%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>43</td>\n",
       "      <td>86.24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>44</td>\n",
       "      <td>86.32%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>45</td>\n",
       "      <td>86.24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>46</td>\n",
       "      <td>86.41%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>86.24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>86.24%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>86.41%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Iteration Number (T_steps) Majority Vote Accuracy\n",
       "0                            0                 75.98%\n",
       "1                            1                 86.92%\n",
       "2                            2                 86.75%\n",
       "3                            3                 86.58%\n",
       "4                            4                 86.58%\n",
       "5                            5                 86.58%\n",
       "6                            6                 86.32%\n",
       "7                            7                 86.32%\n",
       "8                            8                 86.58%\n",
       "9                            9                 86.07%\n",
       "10                          10                 86.15%\n",
       "11                          11                 86.41%\n",
       "12                          12                 86.24%\n",
       "13                          13                 86.41%\n",
       "14                          14                 86.24%\n",
       "15                          15                 86.24%\n",
       "16                          16                 86.41%\n",
       "17                          17                 85.98%\n",
       "18                          18                 86.41%\n",
       "19                          19                 86.15%\n",
       "20                          20                 86.32%\n",
       "21                          21                 86.07%\n",
       "22                          22                 86.24%\n",
       "23                          23                 86.15%\n",
       "24                          24                 86.07%\n",
       "25                          25                 86.41%\n",
       "26                          26                 85.98%\n",
       "27                          27                 86.32%\n",
       "28                          28                 86.24%\n",
       "29                          29                 86.24%\n",
       "30                          30                 86.15%\n",
       "31                          31                 86.24%\n",
       "32                          32                 86.41%\n",
       "33                          33                 86.24%\n",
       "34                          34                 86.41%\n",
       "35                          35                 86.15%\n",
       "36                          36                 86.24%\n",
       "37                          37                 86.41%\n",
       "38                          38                 86.24%\n",
       "39                          39                 86.41%\n",
       "40                          40                 86.32%\n",
       "41                          41                 86.15%\n",
       "42                          42                 86.32%\n",
       "43                          43                 86.24%\n",
       "44                          44                 86.32%\n",
       "45                          45                 86.24%\n",
       "46                          46                 86.41%\n",
       "47                          47                 86.24%\n",
       "48                          48                 86.24%\n",
       "49                          49                 86.41%"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "majority_vote_accuracies = []\n",
    "for t_step_val in range(50):\n",
    "    discriminator_dict_update =  batch_update_discriminators(\n",
    "        discriminator_dict,\n",
    "        choice_df = df_oqwen, \n",
    "        batch_size=8,\n",
    "        T_steps=t_step_val,\n",
    "        learning_rate=0.1\n",
    "    )\n",
    "        \n",
    "    combined_df_refine_update, results_df_refine_update = evaluate_majority_vote(\n",
    "        discriminator_dict_update,  \n",
    "        answer_col='updated_answer_letter', \n",
    "        label_col='answer_letter')\n",
    "\n",
    "\n",
    "\n",
    "    last_row = results_df_refine_update.iloc[-1]\n",
    "    majority_vote_accuracy = last_row['Accuracy']\n",
    "    # print(majority_vote_accuracy)\n",
    "    majority_vote_accuracies.append(majority_vote_accuracy)\n",
    "\n",
    "iteration_numbers = list(range(0, 50))\n",
    "plot_df = pd.DataFrame({\n",
    "    'Iteration Number (T_steps)': iteration_numbers,\n",
    "    'Majority Vote Accuracy': majority_vote_accuracies\n",
    "})\n",
    "plot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799cc4fa",
   "metadata": {},
   "source": [
    "## 7 discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ae2446f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_discriminator_7dict = {\n",
    "    \"OQwen\": temp_df_oqwen,\n",
    "    \"DeepseekQwen\": temp_df_qwen,\n",
    "    \"DeepseekLlama\": temp_df_llama2,\n",
    "    \"Gemma_7B\":  temp_df_gemma_7B,\n",
    "    \"Mistral_7B\":  temp_df_mistral_7B,\n",
    "    \"Ai_Yi_9B\": temp_df_ai_Yi_9B,\n",
    "    \"Openchat_7B\": temp_df_openchat_7B,\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "discriminator_7dict = {\n",
    "    \"OQwen\": df_oqwen,\n",
    "    \"DeepseekQwen\": df_deepseekqwen,\n",
    "    \"DeepseekLlama\": df_deepseekllama,\n",
    "    \"Gemma_7B\": df_gemma_7B,\n",
    "    \"Mistral_7B\": df_mistral_7B,\n",
    "    \"Ai_Yi_9B\": df_ai_Yi_9B,\n",
    "    \"Openchat_7B\": df_openchat_7B,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28cbb4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Discriminator Policy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>1170</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>970</td>\n",
       "      <td>82.91%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DeepseekQwen Accuracy</td>\n",
       "      <td>669</td>\n",
       "      <td>57.18%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DeepseekLlama Accuracy</td>\n",
       "      <td>700</td>\n",
       "      <td>59.83%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gemma_7B Accuracy</td>\n",
       "      <td>811</td>\n",
       "      <td>69.32%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mistral_7B Accuracy</td>\n",
       "      <td>822</td>\n",
       "      <td>70.26%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ai_Yi_9B Accuracy</td>\n",
       "      <td>951</td>\n",
       "      <td>81.28%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Openchat_7B Accuracy</td>\n",
       "      <td>925</td>\n",
       "      <td>79.06%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Majority Vote (&gt;3 models) Accuracy</td>\n",
       "      <td>895</td>\n",
       "      <td>76.50%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions   1170     100%\n",
       "1                      OQwen Accuracy    970   82.91%\n",
       "2               DeepseekQwen Accuracy    669   57.18%\n",
       "3              DeepseekLlama Accuracy    700   59.83%\n",
       "4                   Gemma_7B Accuracy    811   69.32%\n",
       "5                 Mistral_7B Accuracy    822   70.26%\n",
       "6                   Ai_Yi_9B Accuracy    951   81.28%\n",
       "7                Openchat_7B Accuracy    925   79.06%\n",
       "8  Majority Vote (>3 models) Accuracy    895   76.50%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Consensus Game Policy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>1170</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>1025</td>\n",
       "      <td>87.61%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DeepseekQwen Accuracy</td>\n",
       "      <td>822</td>\n",
       "      <td>70.26%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DeepseekLlama Accuracy</td>\n",
       "      <td>741</td>\n",
       "      <td>63.33%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gemma_7B Accuracy</td>\n",
       "      <td>796</td>\n",
       "      <td>68.03%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mistral_7B Accuracy</td>\n",
       "      <td>854</td>\n",
       "      <td>72.99%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ai_Yi_9B Accuracy</td>\n",
       "      <td>1022</td>\n",
       "      <td>87.35%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Openchat_7B Accuracy</td>\n",
       "      <td>984</td>\n",
       "      <td>84.10%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Majority Vote (&gt;3 models) Accuracy</td>\n",
       "      <td>954</td>\n",
       "      <td>81.54%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions   1170     100%\n",
       "1                      OQwen Accuracy   1025   87.61%\n",
       "2               DeepseekQwen Accuracy    822   70.26%\n",
       "3              DeepseekLlama Accuracy    741   63.33%\n",
       "4                   Gemma_7B Accuracy    796   68.03%\n",
       "5                 Mistral_7B Accuracy    854   72.99%\n",
       "6                   Ai_Yi_9B Accuracy   1022   87.35%\n",
       "7                Openchat_7B Accuracy    984   84.10%\n",
       "8  Majority Vote (>3 models) Accuracy    954   81.54%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Updated PEG Policy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Count</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Total Questions</td>\n",
       "      <td>1170</td>\n",
       "      <td>100%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OQwen Accuracy</td>\n",
       "      <td>944</td>\n",
       "      <td>80.68%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DeepseekQwen Accuracy</td>\n",
       "      <td>871</td>\n",
       "      <td>74.44%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DeepseekLlama Accuracy</td>\n",
       "      <td>865</td>\n",
       "      <td>73.93%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gemma_7B Accuracy</td>\n",
       "      <td>884</td>\n",
       "      <td>75.56%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mistral_7B Accuracy</td>\n",
       "      <td>925</td>\n",
       "      <td>79.06%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ai_Yi_9B Accuracy</td>\n",
       "      <td>943</td>\n",
       "      <td>80.60%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Openchat_7B Accuracy</td>\n",
       "      <td>934</td>\n",
       "      <td>79.83%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Majority Vote (&gt;3 models) Accuracy</td>\n",
       "      <td>957</td>\n",
       "      <td>81.79%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Metric  Count Accuracy\n",
       "0                     Total Questions   1170     100%\n",
       "1                      OQwen Accuracy    944   80.68%\n",
       "2               DeepseekQwen Accuracy    871   74.44%\n",
       "3              DeepseekLlama Accuracy    865   73.93%\n",
       "4                   Gemma_7B Accuracy    884   75.56%\n",
       "5                 Mistral_7B Accuracy    925   79.06%\n",
       "6                   Ai_Yi_9B Accuracy    943   80.60%\n",
       "7                Openchat_7B Accuracy    934   79.83%\n",
       "8  Majority Vote (>3 models) Accuracy    957   81.79%"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_df_inital_results_7dis, results_df_inital_7dis = evaluate_majority_vote(\n",
    "    model_dfs=temp_discriminator_7dict,\n",
    "    answer_col='disc_init_answer', \n",
    "    label_col='answer_letter'\n",
    ")\n",
    "print(\"\\nInitial Discriminator Policy\")\n",
    "display(results_df_inital_7dis)\n",
    "\n",
    "combined_df_inital_results_7ED, results_df_inital_7ED  = evaluate_majority_vote(\n",
    "    model_dfs=discriminator_7dict,\n",
    "     answer_col='ED_consensus', \n",
    "     label_col='answer_letter')\n",
    "\n",
    "print(\"\\nInitial Consensus Game Policy\")\n",
    "display(results_df_inital_7ED)\n",
    "\n",
    "## ARC Challenger Original Qwen as problem selector\n",
    "\n",
    "discriminator_7dict_update =  batch_update_discriminators(\n",
    "    discriminator_7dict,\n",
    "    choice_df = df_oqwen, \n",
    "    batch_size=8,\n",
    "    T_steps=8,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "       \n",
    "combined_df_refine_7update, results_df_refine_7update = evaluate_majority_vote(\n",
    "    discriminator_7dict_update,  \n",
    "    answer_col='updated_answer_letter', \n",
    "    label_col='answer_letter')\n",
    "\n",
    "print(\"\\n Updated PEG Policy\")\n",
    "display(results_df_refine_7update)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

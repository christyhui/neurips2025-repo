{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b7c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np# 在你的项目文件夹中运行\n",
    "import google.protobuf\n",
    "import sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f94b8379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toz015_g_ucla_edu/neurips2025-repo/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from huggingface_hub import login\n",
    "from datasets import load_dataset\n",
    "import gc\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03f6cb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "login(os.environ[\"HF_TOKEN\"])\n",
    "hf_token = os.environ.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9ec8b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1170, 4)\n",
      "Index(['id', 'question', 'choices', 'answerKey'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "arc_data = load_dataset(\"allenai/ai2_arc\", \"ARC-Challenge\", split=\"test\")\n",
    "\n",
    "arc_df = arc_data.to_pandas()\n",
    "arc_df = arc_df.drop_duplicates(subset=['question'])\n",
    "print(arc_df.shape)\n",
    "print(arc_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efdd7c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>choices</th>\n",
       "      <th>answerKey</th>\n",
       "      <th>choices_dic</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mercury_7175875</td>\n",
       "      <td>An astronomer observes that a planet rotates f...</td>\n",
       "      <td>[Planetary density will decrease., Planetary y...</td>\n",
       "      <td>C</td>\n",
       "      <td>{'text': ['Planetary density will decrease.', ...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mercury_SC_409171</td>\n",
       "      <td>A group of engineers wanted to know how differ...</td>\n",
       "      <td>[buildings will be built faster, buildings wil...</td>\n",
       "      <td>B</td>\n",
       "      <td>{'text': ['buildings will be built faster', 'b...</td>\n",
       "      <td>science</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                           question  \\\n",
       "0    Mercury_7175875  An astronomer observes that a planet rotates f...   \n",
       "1  Mercury_SC_409171  A group of engineers wanted to know how differ...   \n",
       "\n",
       "                                             choices answerKey  \\\n",
       "0  [Planetary density will decrease., Planetary y...         C   \n",
       "1  [buildings will be built faster, buildings wil...         B   \n",
       "\n",
       "                                         choices_dic  subject  \n",
       "0  {'text': ['Planetary density will decrease.', ...  science  \n",
       "1  {'text': ['buildings will be built faster', 'b...  science  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arc_df[\"choices_dic\"] = arc_df[\"choices\"]\n",
    "arc_df[\"choices\"] = arc_df[\"choices\"].apply(lambda x: x[\"text\"])\n",
    "arc_df[\"subject\"] = \"science\"\n",
    "arc_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb3c5bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available\n",
    "torch.cuda.empty_cache()\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LogitsProcessorList, MinLengthLogitsProcessor\n",
    "import accelerate\n",
    "print(accelerate.__version__)  # Should show ≥0.26.0\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'\n",
    "\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65c13cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "# 1. INITIAL GENERATOR POLICIES\n",
    "################################\n",
    "\n",
    "def format_subject(subject):\n",
    "    l = subject.split(\"_\")\n",
    "    s = \"\"\n",
    "    for entry in l:\n",
    "        s += \" \" + entry\n",
    "    return s\n",
    "\n",
    "\n",
    "def build_generator_prompt(\n",
    "    subject,\n",
    "    target_question,\n",
    "    target_choices,\n",
    "    get_correct\n",
    "):\n",
    "    prompt = \"The following are multiple choice questions (with answers) about {}.\\n\\n\".format(\n",
    "        format_subject(subject))\n",
    "\n",
    "    prompt += f\"{target_question}\"\n",
    "    for i, c in enumerate(target_choices):\n",
    "        prompt += \"\\n{}\".format(c)\n",
    "        \n",
    "    if get_correct:\n",
    "        prompt += \"\\nAnswer:\"\n",
    "    else:\n",
    "        prompt += \"\\nIncorrect Answer:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def get_generator_answer_probs(model, tokenizer, prompt_text, choices_list):\n",
    "    input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    logits = model(input_ids=input_ids).logits[0, -1]\n",
    "\n",
    "\n",
    "    choices = [f\"{chr(65+i)}\" for i, choice in enumerate(choices_list)]\n",
    "    choice_logits = []\n",
    "    for letter in choices:\n",
    "        token_id = tokenizer(letter, return_tensors=\"pt\").input_ids[0, -1].item()\n",
    "        choice_logits.append(logits[token_id].item())\n",
    "    \n",
    "    \n",
    "    choice_logits = torch.tensor(choice_logits, device=model.device).float()\n",
    "    probs = torch.nn.functional.softmax(choice_logits, dim=0).detach().cpu().numpy()\n",
    "    \n",
    "    \n",
    "    choice_probs =  {choice: prob for choice, prob in zip(choices, probs)}\n",
    "    \n",
    "    return choice_probs\n",
    "\n",
    "\n",
    "\n",
    "def generator_probs(subject, question, choices_list, get_correct, model, tokenizer):\n",
    "    # Generate the letter answer\n",
    "    choices = [f\"{chr(65+i)}. {choice}\" for i, choice in enumerate(choices_list)]\n",
    "\n",
    "    prompt = build_generator_prompt(subject, question, choices, get_correct)\n",
    "    \n",
    "    probs = get_generator_answer_probs(model, tokenizer, prompt, choices_list)\n",
    "    \n",
    "    return probs \n",
    "\n",
    "\n",
    "\n",
    "def get_initial_generator_probs(row, model, tokenizer):\n",
    "    gen_init = {\"correct\": {}, \"incorrect\": {}}\n",
    "    x, y_list, subject = row[\"question\"], row[\"choices\"], row[\"subject\"]\n",
    "    for v in [True, False]:\n",
    "        choices_letter_prob = generator_probs(subject, x, y_list, v, model, tokenizer)\n",
    "        if v:\n",
    "            for key, val in choices_letter_prob.items():\n",
    "                gen_init[\"correct\"][key] = val\n",
    "                #print(gen_init.items())\n",
    "        else:\n",
    "            for key, val in choices_letter_prob.items():\n",
    "                gen_init[\"incorrect\"][key] = val\n",
    "\n",
    "    return gen_init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8de33cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# 2. INITIAL DISCRIMINATOR POLICIES\n",
    "###################################\n",
    "\n",
    "def build_discriminator_prompt(\n",
    "    subject: str,\n",
    "    question: str,\n",
    "    proposed_answer: str\n",
    ") -> str:\n",
    "    \"\"\"Builds a prompt to evaluate answer correctness.\"\"\"\n",
    "    prompt = f\"\"\"You are an expert evaluator of questions about {format_subject(subject)}. \n",
    "Determine if the proposed answer is correct. Output ONLY 'A' or 'B'.\n",
    "Question: {question}\n",
    "Proposed Answer: {proposed_answer}\n",
    "\n",
    "Is this answer correct? Respond ONLY with:\n",
    "A. Correct\n",
    "B. Incorrect\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "    \n",
    "def get_discriminator_probs(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt_text,\n",
    "    choices_list\n",
    "):\n",
    "    input_ids = input_ids = tokenizer(prompt_text, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "    \n",
    "    logits = model(input_ids=input_ids).logits[0, -1]\n",
    "\n",
    "    choice_logits = torch.tensor(\n",
    "        [\n",
    "            logits[tokenizer(\"A\").input_ids[-1]],\n",
    "            logits[tokenizer(\"B\").input_ids[-1]],\n",
    "        ]\n",
    "    ).float()\n",
    "    \n",
    "    disc_dict = {\"A\":\"correct\", \"B\":\"incorrect\"}\n",
    "    probs = torch.nn.functional.softmax(choice_logits, dim=0).detach().cpu().numpy()\n",
    "\n",
    "    choices = [f\"{chr(65+i)}\" for i, choice in enumerate(choices_list)]\n",
    "    choice_probs =  {disc_dict[choice]: prob for choice, prob in zip(choices, probs)}\n",
    "\n",
    "    return choice_probs\n",
    "\n",
    "\n",
    "def evaluate_answer_correctness(\n",
    "    row,\n",
    "    model,\n",
    "    tokenizer\n",
    "):\n",
    "    \"\"\"Evaluates all possible answers for a question.\"\"\"\n",
    "    subject = row[\"subject\"]\n",
    "    question = row[\"question\"]\n",
    "    choices = row[\"choices\"]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for idx, answer in enumerate(choices):\n",
    "        prompt = build_discriminator_prompt(\n",
    "            subject=subject,\n",
    "            question=question,\n",
    "            proposed_answer=f\"{answer}\"\n",
    "        )\n",
    "        \n",
    "        probs = get_discriminator_probs(model, tokenizer, prompt, choices)\n",
    "        \n",
    "        \n",
    "        disc_dict_answer =  {i: f\"{chr(65+i)}\" for i, choice in enumerate(row[\"choices\"])}\n",
    "        \n",
    "        \n",
    "        results[disc_dict_answer[idx]] = probs\n",
    "    \n",
    "\n",
    "    return results\n",
    "\n",
    "def get_initial_discriminator_probs(\n",
    "    row,\n",
    "    model,\n",
    "    tokenizer\n",
    "):\n",
    "    disc_init = evaluate_answer_correctness(row, model, tokenizer)\n",
    "    \n",
    "\n",
    "    return disc_init\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8675f26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_answer(gen, disc, candidates, method=\"generator\"):\n",
    "    \"\"\"\n",
    "    method='generator': pick argmax_y pi_G(correct|y)\n",
    "    method='discriminator': pick argmax_y pi_D(correct|y)\n",
    "    \"\"\"\n",
    "    if method == \"generator\":\n",
    "        # For each candidate y, we look at gen[\"correct\"][y].\n",
    "        best_y = None\n",
    "        best_prob = -1.0\n",
    "        for y in candidates:\n",
    "            p = gen[\"correct\"][y]\n",
    "            if p > best_prob:\n",
    "                best_prob = p\n",
    "                best_y = y\n",
    "        return best_y\n",
    "    else:\n",
    "        # method='discriminator'\n",
    "        best_y = None\n",
    "        best_prob = -1.0\n",
    "        for y in candidates:\n",
    "            p = disc[y][\"correct\"]\n",
    "            if p > best_prob:\n",
    "                best_prob = p\n",
    "                best_y = y\n",
    "        return best_y\n",
    "\n",
    "    \n",
    "\n",
    "def softmax(arr):\n",
    "    \"\"\"Numerically stable softmax over a 1D numpy array.\"\"\"\n",
    "    m = np.max(arr)\n",
    "    exp_vals = np.exp(arr - m)\n",
    "    return exp_vals / np.sum(exp_vals)\n",
    "\n",
    "\n",
    "def equilibrium_search(gen_init, disc_init, \n",
    "                       candidates, \n",
    "                       T=5000, \n",
    "                       eta_G=0.1, eta_D=0.1, \n",
    "                       lam_G=0.1, lam_D=0.01):\n",
    "    \"\"\"\n",
    "    Runs iterative no-regret policy updates to find approximate equilibrium.\n",
    "    gen_init, disc_init: dictionary form from the above initialization steps.\n",
    "    \"\"\"\n",
    "    # Convert these dicts into np arrays for speed if you like.\n",
    "    # But for clarity, we'll just keep dict form.\n",
    "\n",
    "    gen = {\"correct\": dict(gen_init[\"correct\"]), \n",
    "           \"incorrect\": dict(gen_init[\"incorrect\"])}\n",
    "    disc = {}\n",
    "    for y in candidates:\n",
    "        disc[y] = dict(disc_init[y])  # copy\n",
    "\n",
    "    Qg = {\"correct\": {y: 0.0 for y in candidates}, \n",
    "          \"incorrect\": {y: 0.0 for y in candidates}}\n",
    "    Qd = {y: {\"correct\": 0.0, \"incorrect\": 0.0} for y in candidates}\n",
    "\n",
    "    for t in range(1, T+1):\n",
    "        # 1) Update Q\n",
    "        for v in [\"correct\", \"incorrect\"]:\n",
    "            for y in candidates:\n",
    "                \n",
    "                Qg[v][y] += (1.0/(2.0*t)) * disc[y][v]\n",
    "\n",
    "        for y in candidates:\n",
    "            for v in [\"correct\", \"incorrect\"]:\n",
    "                \n",
    "                Qd[y][v] += (1.0/(2.0*t)) * gen[v][y]\n",
    "\n",
    "        # 2) Update generator policy\n",
    "        for v in [\"correct\", \"incorrect\"]:\n",
    "            logits = []\n",
    "            for y in candidates:\n",
    "                val = (Qg[v][y] + lam_G * math.log(gen_init[v][y] + 1e-12) )/ (1/eta_G  + lam_G)\n",
    "                logits.append(val)\n",
    "\n",
    "            new_probs = softmax(np.array(logits))\n",
    "\n",
    "            for i, y in enumerate(candidates):\n",
    "                gen[v][y] = new_probs[i]\n",
    "        logits_correct = []\n",
    "        logits_incorrect = []\n",
    "        for y in candidates:\n",
    "            # Logit for \"correct\"\n",
    "            val_correct = (Qd[y][\"correct\"] + lam_D * math.log(disc_init[y][\"correct\"] + 1e-12)) / (1/eta_D + lam_D)\n",
    "            logits_correct.append(val_correct)\n",
    "\n",
    "            # Logit for \"incorrect\"\n",
    "            val_incorrect = (Qd[y][\"incorrect\"] + lam_D * math.log(disc_init[y][\"incorrect\"] + 1e-12)) / (1/eta_D + lam_D)\n",
    "            logits_incorrect.append(val_incorrect)\n",
    "\n",
    "        # Apply softmax across all candidates for each class\n",
    "        new_probs_correct = softmax(np.array(logits_correct))\n",
    "        new_probs_incorrect = softmax(np.array(logits_incorrect))\n",
    "\n",
    "\n",
    "        for i, y in enumerate(candidates):\n",
    "            disc[y][\"correct\"] = new_probs_correct[i]\n",
    "            disc[y][\"incorrect\"] = new_probs_incorrect[i]\n",
    "\n",
    "    return gen, disc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf89481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_model(model_name):\n",
    "    \"\"\"Load one model at a time with 4-bit quantization\"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        load_in_8bit=False,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"cuda\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    return model, tokenizer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9efebe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def subcategory_df_function(model_d, tokenizer_d, df):\n",
    "    \n",
    "    category_df = df.copy()\n",
    "\n",
    "    gen_answer = []\n",
    "    disc_answer = []\n",
    "    gen_init_answer = []\n",
    "    disc_init_answer = []\n",
    "    disc_init_policy = []\n",
    "    gen_init_policy = []\n",
    "    \n",
    "    disc_init_policy = []\n",
    "    gen_init_policy = []\n",
    "    \n",
    "    disc_final_policy_consensus = []\n",
    "    gen_final_policy_consensus = []\n",
    "    \n",
    "\n",
    "    for _, row in tqdm(category_df.iterrows(), total=len(category_df)):\n",
    "\n",
    "        disc_init = get_initial_discriminator_probs(row, model_d, tokenizer_d)\n",
    "        disc_init_policy.append(disc_init)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()  \n",
    "        gen_init = get_initial_generator_probs(row, model_d, tokenizer_d)\n",
    "        \n",
    "        gen_init_policy.append(gen_init)\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        gen_init_answer.append(max(gen_init[\"correct\"], key=gen_init[\"correct\"].get))\n",
    "       \n",
    "        disc_init_answer.append(max(disc_init, key=lambda choice: disc_init[choice][\"correct\"]))\n",
    "        \n",
    "        candidates =  [f\"{chr(65+i)}\" for i, choice in enumerate(row[\"choices\"])]\n",
    "\n",
    "\n",
    "        gen_final, disc_final = equilibrium_search(\n",
    "            gen_init, disc_init, candidates,\n",
    "            T=20, eta_G=0.1, eta_D=0.1, lam_G=0.1, lam_D=0.1\n",
    "        )\n",
    "        disc_final_policy_consensus.append( disc_final)\n",
    "        gen_final_policy_consensus.append(gen_final)\n",
    "\n",
    "        best_answer_g = pick_answer(gen_final, disc_final, candidates, method=\"generator\")\n",
    "        best_answer_d = pick_answer(gen_final, disc_final, candidates, method=\"discriminator\")\n",
    "        \n",
    "        gen_answer.append(best_answer_g)\n",
    "        disc_answer.append(best_answer_d)\n",
    "    \n",
    "    \n",
    "    category_df[\"gen_init_answer\"] = gen_init_answer\n",
    "    category_df[\"disc_answer\"] = disc_answer\n",
    "    category_df[\"gen_answer\"] = gen_answer\n",
    "    category_df[\"disc_init_answer\"] = disc_init_answer\n",
    "    category_df[\"disc_final_policy_consensus\"] = disc_final_policy_consensus\n",
    "    category_df[\"disc_init_policy\"] = disc_init_policy\n",
    "    category_df[\"gen_init_policy\"] = gen_init_policy\n",
    "    category_df[\"gen_final_policy_consensus\"] = gen_final_policy_consensus\n",
    "\n",
    "    \n",
    "    return category_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe078ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/4 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mtokenizer_d\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mglobals\u001b[39m():\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m tokenizer_d\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m model_d, tokenizer_d = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta-llama/Llama-3.1-8B-Instruct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(model_name)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_model\u001b[39m(model_name):\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load one model at a time with 4-bit quantization\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_in_8bit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, tokenizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/neurips2025-repo/venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:600\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    599\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    602\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    604\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    605\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    606\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/neurips2025-repo/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:315\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    313\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    314\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    317\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/neurips2025-repo/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:4998\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4988\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4989\u001b[39m         torch.set_default_dtype(dtype_orig)\n\u001b[32m   4991\u001b[39m     (\n\u001b[32m   4992\u001b[39m         model,\n\u001b[32m   4993\u001b[39m         missing_keys,\n\u001b[32m   4994\u001b[39m         unexpected_keys,\n\u001b[32m   4995\u001b[39m         mismatched_keys,\n\u001b[32m   4996\u001b[39m         offload_index,\n\u001b[32m   4997\u001b[39m         error_msgs,\n\u001b[32m-> \u001b[39m\u001b[32m4998\u001b[39m     ) = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5004\u001b[39m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5005\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5007\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5008\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5014\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5015\u001b[39m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[32m   5016\u001b[39m model.tie_weights()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/neurips2025-repo/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:5456\u001b[39m, in \u001b[36mPreTrainedModel._load_pretrained_model\u001b[39m\u001b[34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[39m\n\u001b[32m   5453\u001b[39m         args_list = logging.tqdm(args_list, desc=\u001b[33m\"\u001b[39m\u001b[33mLoading checkpoint shards\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5455\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m args_list:\n\u001b[32m-> \u001b[39m\u001b[32m5456\u001b[39m         _error_msgs, disk_offload_index, cpu_offload_index = \u001b[43mload_shard_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5457\u001b[39m         error_msgs += _error_msgs\n\u001b[32m   5459\u001b[39m \u001b[38;5;66;03m# Adjust offloaded weights name and save if needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/neurips2025-repo/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:937\u001b[39m, in \u001b[36mload_shard_file\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m    935\u001b[39m \u001b[38;5;66;03m# Skip it with fsdp on ranks other than 0\u001b[39;00m\n\u001b[32m    936\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (is_fsdp_enabled() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_local_dist_rank_0() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quantized):\n\u001b[32m--> \u001b[39m\u001b[32m937\u001b[39m     disk_offload_index, cpu_offload_index = \u001b[43m_load_state_dict_into_meta_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_to_load\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshard_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreverse_key_renaming_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisk_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcpu_offload_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_offloaded_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m        \u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43munexpected_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m error_msgs, disk_offload_index, cpu_offload_index\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/neurips2025-repo/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/neurips2025-repo/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:814\u001b[39m, in \u001b[36m_load_state_dict_into_meta_model\u001b[39m\u001b[34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[39m\n\u001b[32m    812\u001b[39m param = param[...]\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m casting_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m     param = \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasting_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m to_contiguous:\n\u001b[32m    816\u001b[39m     param = param.contiguous()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if 'model_d' in globals():\n",
    "    del model_d\n",
    "\n",
    "if 'tokenizer_d'in globals():\n",
    "    del tokenizer_d\n",
    "model_d, tokenizer_d = load_model(\"meta-llama/Llama-3.1-8B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a803f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "temp_df_llama2 = subcategory_df_function(model_d, tokenizer_d, arc_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded9892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867b7643",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../Data/arc_policy_df_Llama3_8b.csv'\n",
    "temp_df_llama2.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4efcef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if 'model_d' in globals():\n",
    "    del model_d\n",
    "\n",
    "if 'tokenizer_d'in globals():\n",
    "    del tokenizer_d\n",
    "model_d, tokenizer_d = load_model(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "\n",
    "temp_df_Mistral_7B = subcategory_df_function(model_d, tokenizer_d, arc_df)\n",
    "\n",
    "\n",
    "file_path = '../Data/arc_policy_df_Mistral-7B-Instruct.csv'\n",
    "temp_df_Mistral_7B.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05c16c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if 'model_d' in globals():\n",
    "    del model_d\n",
    "\n",
    "if 'tokenizer_d'in globals():\n",
    "    del tokenizer_d\n",
    "model_d, tokenizer_d = load_model(\"google/gemma-7b-it\")\n",
    "\n",
    "temp_df_gemma_7b = subcategory_df_function( model_d, tokenizer_d, arc_df)\n",
    "\n",
    "file_path = '../Data/arc_policy_df_gemma-7b-it.csv'\n",
    "temp_df_gemma_7b.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03b4df88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [01:17<00:00, 19.41s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [01:05<00:00, 16.42s/it]\n",
      "100%|██████████| 1170/1170 [22:08<00:00,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if 'model_d' in globals():\n",
    "    del model_d\n",
    "\n",
    "if 'tokenizer_d'in globals():\n",
    "    del tokenizer_d\n",
    "\n",
    "model_d, tokenizer_d = load_model(\"01-ai/Yi-1.5-9B-Chat\")\n",
    "\n",
    "temp_df_ai_Yi_9B = subcategory_df_function( model_d, tokenizer_d, arc_df)\n",
    "\n",
    "file_path = '../Data/arc_policy_df_ai_Yi_9B.csv'\n",
    "temp_df_ai_Yi_9B.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56799913",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [01:47<00:00, 26.94s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:54<00:00, 13.68s/it]\n",
      "100%|██████████| 1170/1170 [21:00<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if 'model_d' in globals():\n",
    "    del model_d\n",
    "\n",
    "if 'tokenizer_d'in globals():\n",
    "    del tokenizer_d\n",
    "\n",
    "model_d, tokenizer_d = load_model(\"ibm-granite/granite-3.3-8b-base\")\n",
    "\n",
    "temp_df_granite = subcategory_df_function( model_d, tokenizer_d, arc_df)\n",
    "\n",
    "file_path = '../Data/arc_policy_df_granite.csv'\n",
    "temp_df_granite.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4caabaa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 8 files: 100%|██████████| 8/8 [02:27<00:00, 18.50s/it] \n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:55<00:00,  6.90s/it]\n",
      "100%|██████████| 1170/1170 [18:05<00:00,  1.08it/s]\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "if 'model_d' in globals():\n",
    "    del model_d\n",
    "\n",
    "if 'tokenizer_d'in globals():\n",
    "    del tokenizer_d\n",
    "\n",
    "model_d, tokenizer_d = load_model(\"HuggingFaceH4/zephyr-7b-beta\")\n",
    "\n",
    "temp_df_zephyr = subcategory_df_function( model_d, tokenizer_d, arc_df)\n",
    "\n",
    "file_path = '../Data/arc_policy_df_zephyr_7B.csv'\n",
    "temp_df_zephyr.to_csv(file_path, index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebf518b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python peg (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
